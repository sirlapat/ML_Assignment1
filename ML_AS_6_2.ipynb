{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "ML_AS_6.2.ipynb",
      "authorship_tag": "ABX9TyMhAu9Ud+4DRYHohX5J8V4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirlapat/ML_Assignment1/blob/main/ML_AS_6_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p43BqL_WkjMg",
        "outputId": "5cfd99d5-5411-4c4c-e326-1b6bedc41f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.41)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.12)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n",
            "time: 440 µs (started: 2023-12-01 22:49:30 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "UB1MtL66kmJb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imde0d2AkrRO",
        "outputId": "96c86c37-09ad-48ed-9ff7-73af278758d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c6b401d58f0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvNXXpDgkvZ5",
        "outputId": "5d9b3271-3c45-4c17-a73b-3057bfce4dbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 80160044.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XSxBgi-kx_g",
        "outputId": "36680303-f1ec-42d0-c077-c186e2437ac5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4914, 0.4822, 0.4465])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "std"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCTjwqeak2Xa",
        "outputId": "1107c6ec-5835-4582-cb82-1d7ac82eb9c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.2470, 0.2435, 0.2616])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "AzDQVb2UlDRx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "amQBBuEGFH8g",
        "outputId": "5f81e6c4-714b-4a2a-b1bb-abee4cd75c93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "])"
      ],
      "metadata": {
        "id": "x65hCM9mk8Uw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    './data', train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "uc9VDA7zlLdd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10_val = datasets.CIFAR10(\n",
        "     './data', train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "vUMhkt7TlMAU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_image, label = cifar10[0]\n",
        "print(first_image.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0aekSDglOda",
        "outputId": "9849c66a-5b52-4f43-9129-4a42fe2207a6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(cifar10, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(cifar10_val, batch_size=32, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "-UjWjazNlQzE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'device' is defined previously\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model definition\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 512)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "4Y8nGHPmR2lg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_model(model, train_loader, test_loader, num_epochs=300, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Testing the model\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_predicted = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                all_predicted.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        accuracy = correct / total\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Classification Report\n",
        "    report = classification_report(all_labels, all_predicted)\n",
        "    print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "jE7zTrksoN6_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, test_loader, num_epochs=300, lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdFerFDSlcrF",
        "outputId": "367f7923-8662-45a9-ee0c-fcea4722ac17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 1.7886348081870638, Test Accuracy: 40.94%\n",
            "Epoch 2/300, Loss: 1.6521172705020037, Test Accuracy: 43.15%\n",
            "Epoch 3/300, Loss: 1.5804378465437692, Test Accuracy: 44.45%\n",
            "Epoch 4/300, Loss: 1.5198366514246813, Test Accuracy: 45.26%\n",
            "Epoch 5/300, Loss: 1.4625313013544161, Test Accuracy: 46.44%\n",
            "Epoch 6/300, Loss: 1.4085316710646, Test Accuracy: 47.50%\n",
            "Epoch 7/300, Loss: 1.3539841107580803, Test Accuracy: 47.11%\n",
            "Epoch 8/300, Loss: 1.3031534532744078, Test Accuracy: 47.23%\n",
            "Epoch 9/300, Loss: 1.249340126015632, Test Accuracy: 48.36%\n",
            "Epoch 10/300, Loss: 1.1990976107097633, Test Accuracy: 47.90%\n",
            "Epoch 11/300, Loss: 1.147438968516891, Test Accuracy: 48.39%\n",
            "Epoch 12/300, Loss: 1.0958708692496966, Test Accuracy: 49.10%\n",
            "Epoch 13/300, Loss: 1.0460041413990564, Test Accuracy: 48.73%\n",
            "Epoch 14/300, Loss: 0.9943198018247931, Test Accuracy: 48.12%\n",
            "Epoch 15/300, Loss: 0.9466876947223873, Test Accuracy: 48.06%\n",
            "Epoch 16/300, Loss: 0.8974496146195681, Test Accuracy: 48.03%\n",
            "Epoch 17/300, Loss: 0.8493160491216968, Test Accuracy: 48.20%\n",
            "Epoch 18/300, Loss: 0.8030849673087522, Test Accuracy: 47.89%\n",
            "Epoch 19/300, Loss: 0.7584418158308482, Test Accuracy: 47.59%\n",
            "Epoch 20/300, Loss: 0.7151811761079655, Test Accuracy: 47.85%\n",
            "Epoch 21/300, Loss: 0.6756449853161246, Test Accuracy: 46.26%\n",
            "Epoch 22/300, Loss: 0.6319736301021857, Test Accuracy: 48.21%\n",
            "Epoch 23/300, Loss: 0.5908622399439662, Test Accuracy: 47.18%\n",
            "Epoch 24/300, Loss: 0.5566875617700895, Test Accuracy: 47.14%\n",
            "Epoch 25/300, Loss: 0.518112089501614, Test Accuracy: 46.81%\n",
            "Epoch 26/300, Loss: 0.48450431075144945, Test Accuracy: 46.79%\n",
            "Epoch 27/300, Loss: 0.4544286360381432, Test Accuracy: 46.92%\n",
            "Epoch 28/300, Loss: 0.4219303291364885, Test Accuracy: 47.37%\n",
            "Epoch 29/300, Loss: 0.3913330497912543, Test Accuracy: 46.04%\n",
            "Epoch 30/300, Loss: 0.36696073348066094, Test Accuracy: 47.30%\n",
            "Epoch 31/300, Loss: 0.3374213149731768, Test Accuracy: 46.83%\n",
            "Epoch 32/300, Loss: 0.31385827586483817, Test Accuracy: 46.64%\n",
            "Epoch 33/300, Loss: 0.29224866034697816, Test Accuracy: 46.86%\n",
            "Epoch 34/300, Loss: 0.2670081588925266, Test Accuracy: 46.87%\n",
            "Epoch 35/300, Loss: 0.24813721908622266, Test Accuracy: 46.66%\n",
            "Epoch 36/300, Loss: 0.23375389504264885, Test Accuracy: 45.88%\n",
            "Epoch 37/300, Loss: 0.2140110737321778, Test Accuracy: 44.58%\n",
            "Epoch 38/300, Loss: 0.1977330784617863, Test Accuracy: 46.29%\n",
            "Epoch 39/300, Loss: 0.18204637396644494, Test Accuracy: 45.95%\n",
            "Epoch 40/300, Loss: 0.16695387970587036, Test Accuracy: 46.55%\n",
            "Epoch 41/300, Loss: 0.15741203715089264, Test Accuracy: 45.88%\n",
            "Epoch 42/300, Loss: 0.14299652590079714, Test Accuracy: 46.73%\n",
            "Epoch 43/300, Loss: 0.13252591986926565, Test Accuracy: 47.06%\n",
            "Epoch 44/300, Loss: 0.12256554260692647, Test Accuracy: 46.26%\n",
            "Epoch 45/300, Loss: 0.11413938269109697, Test Accuracy: 46.65%\n",
            "Epoch 46/300, Loss: 0.10578923942479504, Test Accuracy: 46.28%\n",
            "Epoch 47/300, Loss: 0.09701926458772374, Test Accuracy: 46.52%\n",
            "Epoch 48/300, Loss: 0.09125530832016308, Test Accuracy: 46.09%\n",
            "Epoch 49/300, Loss: 0.08323960484351703, Test Accuracy: 45.85%\n",
            "Epoch 50/300, Loss: 0.07793031386096777, Test Accuracy: 46.47%\n",
            "Epoch 51/300, Loss: 0.07340179563703174, Test Accuracy: 46.87%\n",
            "Epoch 52/300, Loss: 0.06789153587890602, Test Accuracy: 46.78%\n",
            "Epoch 53/300, Loss: 0.06405183663013762, Test Accuracy: 46.37%\n",
            "Epoch 54/300, Loss: 0.0600957814508707, Test Accuracy: 46.46%\n",
            "Epoch 55/300, Loss: 0.05622955678139294, Test Accuracy: 46.63%\n",
            "Epoch 56/300, Loss: 0.05343070523771657, Test Accuracy: 46.30%\n",
            "Epoch 57/300, Loss: 0.050525530729435685, Test Accuracy: 46.39%\n",
            "Epoch 58/300, Loss: 0.04866047613251232, Test Accuracy: 46.61%\n",
            "Epoch 59/300, Loss: 0.04477761479205454, Test Accuracy: 46.35%\n",
            "Epoch 60/300, Loss: 0.04320085038545989, Test Accuracy: 46.14%\n",
            "Epoch 61/300, Loss: 0.04076752209498458, Test Accuracy: 46.08%\n",
            "Epoch 62/300, Loss: 0.03865865762783447, Test Accuracy: 46.75%\n",
            "Epoch 63/300, Loss: 0.03657484882626199, Test Accuracy: 46.46%\n",
            "Epoch 64/300, Loss: 0.03542648075280743, Test Accuracy: 46.50%\n",
            "Epoch 65/300, Loss: 0.033578440731108876, Test Accuracy: 46.75%\n",
            "Epoch 66/300, Loss: 0.03269095509417322, Test Accuracy: 46.60%\n",
            "Epoch 67/300, Loss: 0.030699097605628306, Test Accuracy: 46.80%\n",
            "Epoch 68/300, Loss: 0.02998452520347126, Test Accuracy: 46.49%\n",
            "Epoch 69/300, Loss: 0.028528229944808355, Test Accuracy: 46.47%\n",
            "Epoch 70/300, Loss: 0.02694758165255904, Test Accuracy: 46.21%\n",
            "Epoch 71/300, Loss: 0.02668847565218015, Test Accuracy: 46.48%\n",
            "Epoch 72/300, Loss: 0.025075945568819764, Test Accuracy: 46.66%\n",
            "Epoch 73/300, Loss: 0.02466983015107388, Test Accuracy: 46.84%\n",
            "Epoch 74/300, Loss: 0.02402608683398345, Test Accuracy: 46.53%\n",
            "Epoch 75/300, Loss: 0.023083230435862537, Test Accuracy: 46.30%\n",
            "Epoch 76/300, Loss: 0.022875311340533687, Test Accuracy: 45.97%\n",
            "Epoch 77/300, Loss: 0.021731449574141533, Test Accuracy: 46.55%\n",
            "Epoch 78/300, Loss: 0.021448273174178693, Test Accuracy: 46.52%\n",
            "Epoch 79/300, Loss: 0.020670220698572585, Test Accuracy: 46.26%\n",
            "Epoch 80/300, Loss: 0.0195862539636921, Test Accuracy: 46.81%\n",
            "Epoch 81/300, Loss: 0.019641837506165927, Test Accuracy: 46.74%\n",
            "Epoch 82/300, Loss: 0.01876176614671831, Test Accuracy: 46.37%\n",
            "Epoch 83/300, Loss: 0.01837130212919192, Test Accuracy: 46.63%\n",
            "Epoch 84/300, Loss: 0.01816597857267637, Test Accuracy: 46.64%\n",
            "Epoch 85/300, Loss: 0.017544476555211092, Test Accuracy: 46.46%\n",
            "Epoch 86/300, Loss: 0.016873627706232432, Test Accuracy: 46.45%\n",
            "Epoch 87/300, Loss: 0.017143391363825637, Test Accuracy: 46.52%\n",
            "Epoch 88/300, Loss: 0.016380528962896258, Test Accuracy: 46.51%\n",
            "Epoch 89/300, Loss: 0.015886996532489456, Test Accuracy: 46.32%\n",
            "Epoch 90/300, Loss: 0.015363656431591462, Test Accuracy: 46.62%\n",
            "Epoch 91/300, Loss: 0.014941332523632968, Test Accuracy: 46.39%\n",
            "Epoch 92/300, Loss: 0.01471734142839022, Test Accuracy: 46.36%\n",
            "Epoch 93/300, Loss: 0.014288926734848238, Test Accuracy: 46.58%\n",
            "Epoch 94/300, Loss: 0.014111164306610422, Test Accuracy: 46.30%\n",
            "Epoch 95/300, Loss: 0.013687521494538907, Test Accuracy: 46.27%\n",
            "Epoch 96/300, Loss: 0.013534353128965093, Test Accuracy: 46.33%\n",
            "Epoch 97/300, Loss: 0.013250522929531541, Test Accuracy: 46.49%\n",
            "Epoch 98/300, Loss: 0.012950430891726235, Test Accuracy: 46.56%\n",
            "Epoch 99/300, Loss: 0.012681678208622007, Test Accuracy: 46.68%\n",
            "Epoch 100/300, Loss: 0.012520023269765913, Test Accuracy: 46.49%\n",
            "Epoch 101/300, Loss: 0.012335369779930348, Test Accuracy: 46.60%\n",
            "Epoch 102/300, Loss: 0.012068102038533963, Test Accuracy: 46.37%\n",
            "Epoch 103/300, Loss: 0.011870389203025565, Test Accuracy: 46.45%\n",
            "Epoch 104/300, Loss: 0.011635880608359496, Test Accuracy: 46.39%\n",
            "Epoch 105/300, Loss: 0.011446556186044895, Test Accuracy: 46.22%\n",
            "Epoch 106/300, Loss: 0.011449126720962353, Test Accuracy: 46.39%\n",
            "Epoch 107/300, Loss: 0.011080324867894004, Test Accuracy: 46.44%\n",
            "Epoch 108/300, Loss: 0.01091612842510754, Test Accuracy: 46.53%\n",
            "Epoch 109/300, Loss: 0.010745574411550586, Test Accuracy: 46.41%\n",
            "Epoch 110/300, Loss: 0.010496180533161547, Test Accuracy: 46.47%\n",
            "Epoch 111/300, Loss: 0.01035216191970646, Test Accuracy: 46.69%\n",
            "Epoch 112/300, Loss: 0.010294637492049298, Test Accuracy: 46.31%\n",
            "Epoch 113/300, Loss: 0.010068107112141008, Test Accuracy: 46.57%\n",
            "Epoch 114/300, Loss: 0.009943575087353176, Test Accuracy: 46.46%\n",
            "Epoch 115/300, Loss: 0.009806347389122136, Test Accuracy: 46.58%\n",
            "Epoch 116/300, Loss: 0.009609846772133866, Test Accuracy: 46.71%\n",
            "Epoch 117/300, Loss: 0.009482827100733573, Test Accuracy: 46.40%\n",
            "Epoch 118/300, Loss: 0.009367233553695148, Test Accuracy: 46.42%\n",
            "Epoch 119/300, Loss: 0.009251358316137537, Test Accuracy: 46.31%\n",
            "Epoch 120/300, Loss: 0.00911956567434035, Test Accuracy: 46.59%\n",
            "Epoch 121/300, Loss: 0.00893467967443154, Test Accuracy: 46.32%\n",
            "Epoch 122/300, Loss: 0.00887006637357266, Test Accuracy: 46.53%\n",
            "Epoch 123/300, Loss: 0.008757781855862108, Test Accuracy: 46.43%\n",
            "Epoch 124/300, Loss: 0.008623649449023923, Test Accuracy: 46.31%\n",
            "Epoch 125/300, Loss: 0.008564376571082813, Test Accuracy: 46.44%\n",
            "Epoch 126/300, Loss: 0.008428683381418025, Test Accuracy: 46.05%\n",
            "Epoch 127/300, Loss: 0.008314380125945252, Test Accuracy: 46.52%\n",
            "Epoch 128/300, Loss: 0.008205748831415912, Test Accuracy: 46.30%\n",
            "Epoch 129/300, Loss: 0.008111260352883861, Test Accuracy: 46.58%\n",
            "Epoch 130/300, Loss: 0.007993335385168696, Test Accuracy: 46.71%\n",
            "Epoch 131/300, Loss: 0.007903217812922622, Test Accuracy: 46.24%\n",
            "Epoch 132/300, Loss: 0.00781233190566001, Test Accuracy: 46.54%\n",
            "Epoch 133/300, Loss: 0.00770140274502909, Test Accuracy: 46.56%\n",
            "Epoch 134/300, Loss: 0.0076539544820647286, Test Accuracy: 46.33%\n",
            "Epoch 135/300, Loss: 0.007530021327932056, Test Accuracy: 46.41%\n",
            "Epoch 136/300, Loss: 0.007434082744399983, Test Accuracy: 46.60%\n",
            "Epoch 137/300, Loss: 0.0073594021711019435, Test Accuracy: 46.49%\n",
            "Epoch 138/300, Loss: 0.007285498056122205, Test Accuracy: 46.58%\n",
            "Epoch 139/300, Loss: 0.0071995996166096905, Test Accuracy: 46.53%\n",
            "Epoch 140/300, Loss: 0.007124542555416042, Test Accuracy: 46.42%\n",
            "Epoch 141/300, Loss: 0.00705536986166112, Test Accuracy: 46.52%\n",
            "Epoch 142/300, Loss: 0.006963541256341314, Test Accuracy: 46.42%\n",
            "Epoch 143/300, Loss: 0.0068913316613352805, Test Accuracy: 46.43%\n",
            "Epoch 144/300, Loss: 0.006836607304454727, Test Accuracy: 46.60%\n",
            "Epoch 145/300, Loss: 0.006761622746246828, Test Accuracy: 46.42%\n",
            "Epoch 146/300, Loss: 0.006673188021457775, Test Accuracy: 46.44%\n",
            "Epoch 147/300, Loss: 0.006614029655228258, Test Accuracy: 46.43%\n",
            "Epoch 148/300, Loss: 0.006542946558036935, Test Accuracy: 46.21%\n",
            "Epoch 149/300, Loss: 0.006486058564229808, Test Accuracy: 46.51%\n",
            "Epoch 150/300, Loss: 0.006419555968423246, Test Accuracy: 46.41%\n",
            "Epoch 151/300, Loss: 0.006365720535701788, Test Accuracy: 46.37%\n",
            "Epoch 152/300, Loss: 0.006286170895515881, Test Accuracy: 46.45%\n",
            "Epoch 153/300, Loss: 0.006236795647491677, Test Accuracy: 46.44%\n",
            "Epoch 154/300, Loss: 0.006177555565452364, Test Accuracy: 46.41%\n",
            "Epoch 155/300, Loss: 0.0061134056298995315, Test Accuracy: 46.60%\n",
            "Epoch 156/300, Loss: 0.006044875722115124, Test Accuracy: 46.34%\n",
            "Epoch 157/300, Loss: 0.005995429740945784, Test Accuracy: 46.16%\n",
            "Epoch 158/300, Loss: 0.005953047102643743, Test Accuracy: 46.54%\n",
            "Epoch 159/300, Loss: 0.00588813055216103, Test Accuracy: 46.36%\n",
            "Epoch 160/300, Loss: 0.005829815159034001, Test Accuracy: 46.30%\n",
            "Epoch 161/300, Loss: 0.005788469020788506, Test Accuracy: 46.33%\n",
            "Epoch 162/300, Loss: 0.0057194081509887446, Test Accuracy: 46.42%\n",
            "Epoch 163/300, Loss: 0.0056900763891634945, Test Accuracy: 46.34%\n",
            "Epoch 164/300, Loss: 0.0056314890672734584, Test Accuracy: 46.43%\n",
            "Epoch 165/300, Loss: 0.0055843182532289565, Test Accuracy: 46.34%\n",
            "Epoch 166/300, Loss: 0.005529932334532893, Test Accuracy: 46.25%\n",
            "Epoch 167/300, Loss: 0.005480694986006301, Test Accuracy: 46.41%\n",
            "Epoch 168/300, Loss: 0.005439800962416058, Test Accuracy: 46.35%\n",
            "Epoch 169/300, Loss: 0.00538251282300464, Test Accuracy: 46.50%\n",
            "Epoch 170/300, Loss: 0.005337228367828495, Test Accuracy: 46.44%\n",
            "Epoch 171/300, Loss: 0.005294682984183747, Test Accuracy: 46.41%\n",
            "Epoch 172/300, Loss: 0.005262249360315058, Test Accuracy: 46.17%\n",
            "Epoch 173/300, Loss: 0.005215800102288886, Test Accuracy: 46.34%\n",
            "Epoch 174/300, Loss: 0.005173319725764252, Test Accuracy: 46.43%\n",
            "Epoch 175/300, Loss: 0.0051209048735381835, Test Accuracy: 46.55%\n",
            "Epoch 176/300, Loss: 0.005078459693022401, Test Accuracy: 46.36%\n",
            "Epoch 177/300, Loss: 0.0050439278415499, Test Accuracy: 46.53%\n",
            "Epoch 178/300, Loss: 0.005007016551767262, Test Accuracy: 46.35%\n",
            "Epoch 179/300, Loss: 0.004973149739690864, Test Accuracy: 46.33%\n",
            "Epoch 180/300, Loss: 0.004930987740078522, Test Accuracy: 46.23%\n",
            "Epoch 181/300, Loss: 0.004888740459651289, Test Accuracy: 46.47%\n",
            "Epoch 182/300, Loss: 0.004847669434719984, Test Accuracy: 46.27%\n",
            "Epoch 183/300, Loss: 0.004811947973134021, Test Accuracy: 46.40%\n",
            "Epoch 184/300, Loss: 0.00477013222352433, Test Accuracy: 46.48%\n",
            "Epoch 185/300, Loss: 0.004737283959561722, Test Accuracy: 46.22%\n",
            "Epoch 186/300, Loss: 0.00470068693349897, Test Accuracy: 46.44%\n",
            "Epoch 187/300, Loss: 0.004673335423111506, Test Accuracy: 46.30%\n",
            "Epoch 188/300, Loss: 0.004640165088526662, Test Accuracy: 46.41%\n",
            "Epoch 189/300, Loss: 0.004602413316914527, Test Accuracy: 46.46%\n",
            "Epoch 190/300, Loss: 0.004566066762103544, Test Accuracy: 46.37%\n",
            "Epoch 191/300, Loss: 0.004537050017457806, Test Accuracy: 46.43%\n",
            "Epoch 192/300, Loss: 0.004504614870432795, Test Accuracy: 46.47%\n",
            "Epoch 193/300, Loss: 0.004479275698763433, Test Accuracy: 46.32%\n",
            "Epoch 194/300, Loss: 0.004435529055131059, Test Accuracy: 46.36%\n",
            "Epoch 195/300, Loss: 0.004408082138253608, Test Accuracy: 46.41%\n",
            "Epoch 196/300, Loss: 0.004372622498971907, Test Accuracy: 46.43%\n",
            "Epoch 197/300, Loss: 0.0043425731452078255, Test Accuracy: 46.41%\n",
            "Epoch 198/300, Loss: 0.0043176171528944126, Test Accuracy: 46.32%\n",
            "Epoch 199/300, Loss: 0.004283114826224473, Test Accuracy: 46.40%\n",
            "Epoch 200/300, Loss: 0.004257794865243189, Test Accuracy: 46.43%\n",
            "Epoch 201/300, Loss: 0.00422845135209933, Test Accuracy: 46.52%\n",
            "Epoch 202/300, Loss: 0.004198365622382999, Test Accuracy: 46.29%\n",
            "Epoch 203/300, Loss: 0.00417097257086751, Test Accuracy: 46.39%\n",
            "Epoch 204/300, Loss: 0.004140672800074536, Test Accuracy: 46.41%\n",
            "Epoch 205/300, Loss: 0.004115963240898311, Test Accuracy: 46.32%\n",
            "Epoch 206/300, Loss: 0.004081610707603085, Test Accuracy: 46.42%\n",
            "Epoch 207/300, Loss: 0.0040638568183682475, Test Accuracy: 46.41%\n",
            "Epoch 208/300, Loss: 0.004034158541038585, Test Accuracy: 46.38%\n",
            "Epoch 209/300, Loss: 0.004003579805319698, Test Accuracy: 46.52%\n",
            "Epoch 210/300, Loss: 0.003977131998474499, Test Accuracy: 46.42%\n",
            "Epoch 211/300, Loss: 0.0039572251781282595, Test Accuracy: 46.29%\n",
            "Epoch 212/300, Loss: 0.003933481389662026, Test Accuracy: 46.42%\n",
            "Epoch 213/300, Loss: 0.003908129309000768, Test Accuracy: 46.50%\n",
            "Epoch 214/300, Loss: 0.0038802997332473862, Test Accuracy: 46.43%\n",
            "Epoch 215/300, Loss: 0.0038570784199482362, Test Accuracy: 46.50%\n",
            "Epoch 216/300, Loss: 0.0038284156981692368, Test Accuracy: 46.31%\n",
            "Epoch 217/300, Loss: 0.0038081994126786217, Test Accuracy: 46.43%\n",
            "Epoch 218/300, Loss: 0.003789753064999425, Test Accuracy: 46.13%\n",
            "Epoch 219/300, Loss: 0.0037612617273546246, Test Accuracy: 46.32%\n",
            "Epoch 220/300, Loss: 0.003737788517218767, Test Accuracy: 46.47%\n",
            "Epoch 221/300, Loss: 0.003717697059267313, Test Accuracy: 46.28%\n",
            "Epoch 222/300, Loss: 0.0036967343421949648, Test Accuracy: 46.44%\n",
            "Epoch 223/300, Loss: 0.0036737007834225827, Test Accuracy: 46.27%\n",
            "Epoch 224/300, Loss: 0.003653723528231503, Test Accuracy: 46.33%\n",
            "Epoch 225/300, Loss: 0.0036288713273650093, Test Accuracy: 46.33%\n",
            "Epoch 226/300, Loss: 0.0036061756333404162, Test Accuracy: 46.33%\n",
            "Epoch 227/300, Loss: 0.003588431019047591, Test Accuracy: 46.38%\n",
            "Epoch 228/300, Loss: 0.003568144407083405, Test Accuracy: 46.42%\n",
            "Epoch 229/300, Loss: 0.003544409252641817, Test Accuracy: 46.39%\n",
            "Epoch 230/300, Loss: 0.0035226147203877216, Test Accuracy: 46.37%\n",
            "Epoch 231/300, Loss: 0.0035048010469320746, Test Accuracy: 46.34%\n",
            "Epoch 232/300, Loss: 0.003484587505953154, Test Accuracy: 46.38%\n",
            "Epoch 233/300, Loss: 0.003466156128107529, Test Accuracy: 46.38%\n",
            "Epoch 234/300, Loss: 0.0034434198739569603, Test Accuracy: 46.26%\n",
            "Epoch 235/300, Loss: 0.0034248108621204054, Test Accuracy: 46.41%\n",
            "Epoch 236/300, Loss: 0.003407718024144494, Test Accuracy: 46.39%\n",
            "Epoch 237/300, Loss: 0.003387728432504435, Test Accuracy: 46.42%\n",
            "Epoch 238/300, Loss: 0.0033648654688787256, Test Accuracy: 46.28%\n",
            "Epoch 239/300, Loss: 0.003347438299132872, Test Accuracy: 46.45%\n",
            "Epoch 240/300, Loss: 0.003331032436462006, Test Accuracy: 46.39%\n",
            "Epoch 241/300, Loss: 0.0033131123425692833, Test Accuracy: 46.34%\n",
            "Epoch 242/300, Loss: 0.0032924102924368507, Test Accuracy: 46.36%\n",
            "Epoch 243/300, Loss: 0.0032770960729227175, Test Accuracy: 46.48%\n",
            "Epoch 244/300, Loss: 0.0032582972279165275, Test Accuracy: 46.37%\n",
            "Epoch 245/300, Loss: 0.0032412573656154246, Test Accuracy: 46.44%\n",
            "Epoch 246/300, Loss: 0.003223197243902749, Test Accuracy: 46.34%\n",
            "Epoch 247/300, Loss: 0.0032095514058826046, Test Accuracy: 46.42%\n",
            "Epoch 248/300, Loss: 0.0031895194375012796, Test Accuracy: 46.45%\n",
            "Epoch 249/300, Loss: 0.0031713938045745058, Test Accuracy: 46.33%\n",
            "Epoch 250/300, Loss: 0.003157238603268064, Test Accuracy: 46.50%\n",
            "Epoch 251/300, Loss: 0.0031408669130003138, Test Accuracy: 46.39%\n",
            "Epoch 252/300, Loss: 0.0031252729053855162, Test Accuracy: 46.31%\n",
            "Epoch 253/300, Loss: 0.0031075899632825197, Test Accuracy: 46.28%\n",
            "Epoch 254/300, Loss: 0.0030900551731205443, Test Accuracy: 46.47%\n",
            "Epoch 255/300, Loss: 0.003080192890187448, Test Accuracy: 46.45%\n",
            "Epoch 256/300, Loss: 0.003061778238683734, Test Accuracy: 46.32%\n",
            "Epoch 257/300, Loss: 0.003046404727125661, Test Accuracy: 46.48%\n",
            "Epoch 258/300, Loss: 0.0030268456146177037, Test Accuracy: 46.38%\n",
            "Epoch 259/300, Loss: 0.003014522163413098, Test Accuracy: 46.40%\n",
            "Epoch 260/300, Loss: 0.002998722334775504, Test Accuracy: 46.38%\n",
            "Epoch 261/300, Loss: 0.002982027942352879, Test Accuracy: 46.32%\n",
            "Epoch 262/300, Loss: 0.002969718253331155, Test Accuracy: 46.48%\n",
            "Epoch 263/300, Loss: 0.002952626858071036, Test Accuracy: 46.37%\n",
            "Epoch 264/300, Loss: 0.0029414609968078324, Test Accuracy: 46.42%\n",
            "Epoch 265/300, Loss: 0.0029243383523385427, Test Accuracy: 46.38%\n",
            "Epoch 266/300, Loss: 0.0029117256463746647, Test Accuracy: 46.34%\n",
            "Epoch 267/300, Loss: 0.002897987902368242, Test Accuracy: 46.43%\n",
            "Epoch 268/300, Loss: 0.0028830282789341012, Test Accuracy: 46.35%\n",
            "Epoch 269/300, Loss: 0.002870039763977788, Test Accuracy: 46.48%\n",
            "Epoch 270/300, Loss: 0.002855990785135341, Test Accuracy: 46.41%\n",
            "Epoch 271/300, Loss: 0.00284292120333065, Test Accuracy: 46.34%\n",
            "Epoch 272/300, Loss: 0.0028310952332289085, Test Accuracy: 46.38%\n",
            "Epoch 273/300, Loss: 0.002812286183298807, Test Accuracy: 46.41%\n",
            "Epoch 274/300, Loss: 0.0028044259650406313, Test Accuracy: 46.30%\n",
            "Epoch 275/300, Loss: 0.002790762962873546, Test Accuracy: 46.29%\n",
            "Epoch 276/300, Loss: 0.0027768136607214896, Test Accuracy: 46.37%\n",
            "Epoch 277/300, Loss: 0.002761993092596197, Test Accuracy: 46.41%\n",
            "Epoch 278/300, Loss: 0.002748818992475025, Test Accuracy: 46.36%\n",
            "Epoch 279/300, Loss: 0.002736787100352933, Test Accuracy: 46.40%\n",
            "Epoch 280/300, Loss: 0.0027258174980425123, Test Accuracy: 46.38%\n",
            "Epoch 281/300, Loss: 0.002712623666827725, Test Accuracy: 46.32%\n",
            "Epoch 282/300, Loss: 0.0027022401651639254, Test Accuracy: 46.38%\n",
            "Epoch 283/300, Loss: 0.0026899651115252538, Test Accuracy: 46.32%\n",
            "Epoch 284/300, Loss: 0.00267561457789624, Test Accuracy: 46.42%\n",
            "Epoch 285/300, Loss: 0.0026632369307786827, Test Accuracy: 46.39%\n",
            "Epoch 286/300, Loss: 0.0026527357640600525, Test Accuracy: 46.35%\n",
            "Epoch 287/300, Loss: 0.002643273066701302, Test Accuracy: 46.39%\n",
            "Epoch 288/300, Loss: 0.0026288867753948863, Test Accuracy: 46.35%\n",
            "Epoch 289/300, Loss: 0.002616797818463255, Test Accuracy: 46.39%\n",
            "Epoch 290/300, Loss: 0.0026049305008866144, Test Accuracy: 46.33%\n",
            "Epoch 291/300, Loss: 0.0025932669720921714, Test Accuracy: 46.31%\n",
            "Epoch 292/300, Loss: 0.00258026281845418, Test Accuracy: 46.22%\n",
            "Epoch 293/300, Loss: 0.002571773420517605, Test Accuracy: 46.23%\n",
            "Epoch 294/300, Loss: 0.002561234738868414, Test Accuracy: 46.31%\n",
            "Epoch 295/300, Loss: 0.0025501772036635325, Test Accuracy: 46.37%\n",
            "Epoch 296/300, Loss: 0.0025344548759754374, Test Accuracy: 46.24%\n",
            "Epoch 297/300, Loss: 0.002527197924372874, Test Accuracy: 46.33%\n",
            "Epoch 298/300, Loss: 0.0025166352315235617, Test Accuracy: 46.30%\n",
            "Epoch 299/300, Loss: 0.002505141013583153, Test Accuracy: 46.33%\n",
            "Epoch 300/300, Loss: 0.002496123177825246, Test Accuracy: 46.37%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.53      0.53      1000\n",
            "           1       0.60      0.53      0.56      1000\n",
            "           2       0.34      0.38      0.36      1000\n",
            "           3       0.30      0.29      0.29      1000\n",
            "           4       0.40      0.42      0.41      1000\n",
            "           5       0.34      0.34      0.34      1000\n",
            "           6       0.48      0.52      0.50      1000\n",
            "           7       0.54      0.50      0.52      1000\n",
            "           8       0.58      0.64      0.61      1000\n",
            "           9       0.55      0.50      0.52      1000\n",
            "\n",
            "    accuracy                           0.46     10000\n",
            "   macro avg       0.47      0.46      0.46     10000\n",
            "weighted avg       0.47      0.46      0.46     10000\n",
            "\n",
            "time: 2h 35min 51s (started: 2023-12-01 22:53:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 32 * 32 * 3\n",
        "hidden_sizes = [512, 256, 128]\n",
        "output_size = 10\n",
        "\n",
        "layers = []\n",
        "layers.append(nn.Flatten())\n",
        "\n",
        "# Adding hidden layers with Tanh activation function\n",
        "for i in range(len(hidden_sizes)):\n",
        "    layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "    layers.append(nn.Tanh())\n",
        "\n",
        "# Output layer\n",
        "layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "model2 = nn.Sequential(*layers).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxC1uowan54a",
        "outputId": "c1efd027-c2f5-459c-85a9-e477696c57be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 20.1 ms (started: 2023-12-02 01:36:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 32 * 3, 512),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(512, 256),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCUAAKJEniQa",
        "outputId": "5f866906-afa2-4a0d-a22f-6b3bce36a9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 18.1 ms (started: 2023-12-02 01:36:32 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model2, train_loader, test_loader, num_epochs=300, lr=0.02)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha9wugCsMBMI",
        "outputId": "56536605-cb1e-4e30-dc9c-39b80e5f264d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Loss: 1.5464903359526025, Test Accuracy: 45.85%\n",
            "Epoch 2/300, Loss: 1.4621418760285039, Test Accuracy: 46.92%\n",
            "Epoch 3/300, Loss: 1.389218071905833, Test Accuracy: 47.25%\n",
            "Epoch 4/300, Loss: 1.3159023884466956, Test Accuracy: 46.78%\n",
            "Epoch 5/300, Loss: 1.242584570698912, Test Accuracy: 47.96%\n",
            "Epoch 6/300, Loss: 1.16827581127866, Test Accuracy: 47.50%\n",
            "Epoch 7/300, Loss: 1.0966241708262487, Test Accuracy: 48.47%\n",
            "Epoch 8/300, Loss: 1.0286566393923011, Test Accuracy: 48.42%\n",
            "Epoch 9/300, Loss: 0.9540793951207526, Test Accuracy: 46.65%\n",
            "Epoch 10/300, Loss: 0.8861436463668418, Test Accuracy: 48.49%\n",
            "Epoch 11/300, Loss: 0.8212438619106303, Test Accuracy: 47.75%\n",
            "Epoch 12/300, Loss: 0.7507310081809587, Test Accuracy: 46.75%\n",
            "Epoch 13/300, Loss: 0.6914542057692662, Test Accuracy: 46.30%\n",
            "Epoch 14/300, Loss: 0.6358378670837966, Test Accuracy: 47.92%\n",
            "Epoch 15/300, Loss: 0.5780094443741168, Test Accuracy: 47.44%\n",
            "Epoch 16/300, Loss: 0.5192055108927796, Test Accuracy: 47.40%\n",
            "Epoch 17/300, Loss: 0.4773495583315347, Test Accuracy: 46.43%\n",
            "Epoch 18/300, Loss: 0.43381167390048314, Test Accuracy: 46.44%\n",
            "Epoch 19/300, Loss: 0.39272845391558286, Test Accuracy: 47.78%\n",
            "Epoch 20/300, Loss: 0.3522249758672577, Test Accuracy: 45.42%\n",
            "Epoch 21/300, Loss: 0.3169041551001279, Test Accuracy: 47.39%\n",
            "Epoch 22/300, Loss: 0.28231443550559243, Test Accuracy: 47.40%\n",
            "Epoch 23/300, Loss: 0.2528721518523786, Test Accuracy: 47.25%\n",
            "Epoch 24/300, Loss: 0.22765424004943607, Test Accuracy: 47.05%\n",
            "Epoch 25/300, Loss: 0.21080851035522713, Test Accuracy: 46.45%\n",
            "Epoch 26/300, Loss: 0.1882130551442113, Test Accuracy: 46.45%\n",
            "Epoch 27/300, Loss: 0.17314490622775874, Test Accuracy: 45.48%\n",
            "Epoch 28/300, Loss: 0.15708430333543266, Test Accuracy: 45.42%\n",
            "Epoch 29/300, Loss: 0.145294555007663, Test Accuracy: 46.27%\n",
            "Epoch 30/300, Loss: 0.14338401253010435, Test Accuracy: 47.10%\n",
            "Epoch 31/300, Loss: 0.12803049010516013, Test Accuracy: 46.80%\n",
            "Epoch 32/300, Loss: 0.1064071226692255, Test Accuracy: 47.41%\n",
            "Epoch 33/300, Loss: 0.09715667144786888, Test Accuracy: 46.20%\n",
            "Epoch 34/300, Loss: 0.09180640304039256, Test Accuracy: 45.97%\n",
            "Epoch 35/300, Loss: 0.09447442776191639, Test Accuracy: 46.88%\n",
            "Epoch 36/300, Loss: 0.08760574736089582, Test Accuracy: 47.38%\n",
            "Epoch 37/300, Loss: 0.07099713082961566, Test Accuracy: 46.87%\n",
            "Epoch 38/300, Loss: 0.046344512337085836, Test Accuracy: 45.47%\n",
            "Epoch 39/300, Loss: 0.028365964776945377, Test Accuracy: 47.60%\n",
            "Epoch 40/300, Loss: 0.02061448742240272, Test Accuracy: 47.41%\n",
            "Epoch 41/300, Loss: 0.018365855057773276, Test Accuracy: 46.67%\n",
            "Epoch 42/300, Loss: 0.014804045493882543, Test Accuracy: 47.42%\n",
            "Epoch 43/300, Loss: 0.009315812133405458, Test Accuracy: 47.98%\n",
            "Epoch 44/300, Loss: 0.00802174007706061, Test Accuracy: 48.02%\n",
            "Epoch 45/300, Loss: 0.0033904182652891293, Test Accuracy: 48.33%\n",
            "Epoch 46/300, Loss: 0.0021312219518546957, Test Accuracy: 48.29%\n",
            "Epoch 47/300, Loss: 0.001382516545422467, Test Accuracy: 48.31%\n",
            "Epoch 48/300, Loss: 0.001170572362466574, Test Accuracy: 48.29%\n",
            "Epoch 49/300, Loss: 0.001036992318595233, Test Accuracy: 48.23%\n",
            "Epoch 50/300, Loss: 0.000954226850960937, Test Accuracy: 48.18%\n",
            "Epoch 51/300, Loss: 0.0008888424524560872, Test Accuracy: 48.27%\n",
            "Epoch 52/300, Loss: 0.0008376233129865941, Test Accuracy: 48.23%\n",
            "Epoch 53/300, Loss: 0.0007927463951173387, Test Accuracy: 48.20%\n",
            "Epoch 54/300, Loss: 0.0007537372058798349, Test Accuracy: 48.24%\n",
            "Epoch 55/300, Loss: 0.0007163667758740991, Test Accuracy: 48.22%\n",
            "Epoch 56/300, Loss: 0.0006867081706963453, Test Accuracy: 48.27%\n",
            "Epoch 57/300, Loss: 0.0006588902388213284, Test Accuracy: 48.24%\n",
            "Epoch 58/300, Loss: 0.0006314216091549912, Test Accuracy: 48.18%\n",
            "Epoch 59/300, Loss: 0.0006080930125451231, Test Accuracy: 48.26%\n",
            "Epoch 60/300, Loss: 0.0005869479302566567, Test Accuracy: 48.27%\n",
            "Epoch 61/300, Loss: 0.0005669774812560854, Test Accuracy: 48.30%\n",
            "Epoch 62/300, Loss: 0.0005485816777411786, Test Accuracy: 48.28%\n",
            "Epoch 63/300, Loss: 0.000532618053946796, Test Accuracy: 48.32%\n",
            "Epoch 64/300, Loss: 0.0005159571051910196, Test Accuracy: 48.31%\n",
            "Epoch 65/300, Loss: 0.0005009044915458479, Test Accuracy: 48.26%\n",
            "Epoch 66/300, Loss: 0.00048687406541774125, Test Accuracy: 48.23%\n",
            "Epoch 67/300, Loss: 0.00047339383904398816, Test Accuracy: 48.30%\n",
            "Epoch 68/300, Loss: 0.00046161700961481236, Test Accuracy: 48.28%\n",
            "Epoch 69/300, Loss: 0.0004494822669765505, Test Accuracy: 48.20%\n",
            "Epoch 70/300, Loss: 0.00043892241031692975, Test Accuracy: 48.28%\n",
            "Epoch 71/300, Loss: 0.0004279448035593494, Test Accuracy: 48.25%\n",
            "Epoch 72/300, Loss: 0.00041812162289797536, Test Accuracy: 48.32%\n",
            "Epoch 73/300, Loss: 0.0004087099331024121, Test Accuracy: 48.25%\n",
            "Epoch 74/300, Loss: 0.0003992607277664666, Test Accuracy: 48.25%\n",
            "Epoch 75/300, Loss: 0.00039158589915797427, Test Accuracy: 48.26%\n",
            "Epoch 76/300, Loss: 0.0003824629811805784, Test Accuracy: 48.29%\n",
            "Epoch 77/300, Loss: 0.0003747106339284639, Test Accuracy: 48.30%\n",
            "Epoch 78/300, Loss: 0.0003671356711573291, Test Accuracy: 48.41%\n",
            "Epoch 79/300, Loss: 0.00036006672107306224, Test Accuracy: 48.28%\n",
            "Epoch 80/300, Loss: 0.00035341879976453035, Test Accuracy: 48.29%\n",
            "Epoch 81/300, Loss: 0.0003465958280444979, Test Accuracy: 48.21%\n",
            "Epoch 82/300, Loss: 0.00034032064817908535, Test Accuracy: 48.30%\n",
            "Epoch 83/300, Loss: 0.00033398142306305265, Test Accuracy: 48.35%\n",
            "Epoch 84/300, Loss: 0.0003280305944671613, Test Accuracy: 48.26%\n",
            "Epoch 85/300, Loss: 0.0003224301712696741, Test Accuracy: 48.29%\n",
            "Epoch 86/300, Loss: 0.00031695251588922897, Test Accuracy: 48.31%\n",
            "Epoch 87/300, Loss: 0.0003116046029805888, Test Accuracy: 48.33%\n",
            "Epoch 88/300, Loss: 0.00030637739447535905, Test Accuracy: 48.33%\n",
            "Epoch 89/300, Loss: 0.00030137983050080577, Test Accuracy: 48.26%\n",
            "Epoch 90/300, Loss: 0.0002965936081877076, Test Accuracy: 48.28%\n",
            "Epoch 91/300, Loss: 0.0002920028361859353, Test Accuracy: 48.27%\n",
            "Epoch 92/300, Loss: 0.0002877171495465338, Test Accuracy: 48.32%\n",
            "Epoch 93/300, Loss: 0.0002832926225625646, Test Accuracy: 48.32%\n",
            "Epoch 94/300, Loss: 0.0002789377797382627, Test Accuracy: 48.31%\n",
            "Epoch 95/300, Loss: 0.00027501011892934365, Test Accuracy: 48.33%\n",
            "Epoch 96/300, Loss: 0.0002709068633411406, Test Accuracy: 48.32%\n",
            "Epoch 97/300, Loss: 0.0002670672393732562, Test Accuracy: 48.31%\n",
            "Epoch 98/300, Loss: 0.0002632583680502196, Test Accuracy: 48.31%\n",
            "Epoch 99/300, Loss: 0.00025970201982227403, Test Accuracy: 48.30%\n",
            "Epoch 100/300, Loss: 0.00025622348599655016, Test Accuracy: 48.37%\n",
            "Epoch 101/300, Loss: 0.0002528851711720357, Test Accuracy: 48.25%\n",
            "Epoch 102/300, Loss: 0.000249376274742602, Test Accuracy: 48.27%\n",
            "Epoch 103/300, Loss: 0.0002463587055944939, Test Accuracy: 48.27%\n",
            "Epoch 104/300, Loss: 0.0002431409561714102, Test Accuracy: 48.24%\n",
            "Epoch 105/300, Loss: 0.00023998077728579854, Test Accuracy: 48.25%\n",
            "Epoch 106/300, Loss: 0.0002369794001552468, Test Accuracy: 48.27%\n",
            "Epoch 107/300, Loss: 0.00023424331651237032, Test Accuracy: 48.27%\n",
            "Epoch 108/300, Loss: 0.00023115323478895924, Test Accuracy: 48.19%\n",
            "Epoch 109/300, Loss: 0.00022833869258187336, Test Accuracy: 48.24%\n",
            "Epoch 110/300, Loss: 0.00022559434775015343, Test Accuracy: 48.27%\n",
            "Epoch 111/300, Loss: 0.00022292002778657029, Test Accuracy: 48.23%\n",
            "Epoch 112/300, Loss: 0.00022043618945128858, Test Accuracy: 48.21%\n",
            "Epoch 113/300, Loss: 0.00021783691015336883, Test Accuracy: 48.23%\n",
            "Epoch 114/300, Loss: 0.0002154233478859838, Test Accuracy: 48.20%\n",
            "Epoch 115/300, Loss: 0.00021299629577439127, Test Accuracy: 48.21%\n",
            "Epoch 116/300, Loss: 0.00021073295756208387, Test Accuracy: 48.20%\n",
            "Epoch 117/300, Loss: 0.00020836730884652203, Test Accuracy: 48.21%\n",
            "Epoch 118/300, Loss: 0.00020616567290777737, Test Accuracy: 48.19%\n",
            "Epoch 119/300, Loss: 0.00020386810578614108, Test Accuracy: 48.21%\n",
            "Epoch 120/300, Loss: 0.0002017497578319636, Test Accuracy: 48.21%\n",
            "Epoch 121/300, Loss: 0.00019966329274698644, Test Accuracy: 48.22%\n",
            "Epoch 122/300, Loss: 0.00019751149054952007, Test Accuracy: 48.18%\n",
            "Epoch 123/300, Loss: 0.00019549243391368354, Test Accuracy: 48.19%\n",
            "Epoch 124/300, Loss: 0.00019353778418266366, Test Accuracy: 48.19%\n",
            "Epoch 125/300, Loss: 0.00019162542531752432, Test Accuracy: 48.21%\n",
            "Epoch 126/300, Loss: 0.00018973825213311523, Test Accuracy: 48.22%\n",
            "Epoch 127/300, Loss: 0.00018785150139182937, Test Accuracy: 48.21%\n",
            "Epoch 128/300, Loss: 0.0001860093596351234, Test Accuracy: 48.18%\n",
            "Epoch 129/300, Loss: 0.00018420083009331324, Test Accuracy: 48.18%\n",
            "Epoch 130/300, Loss: 0.00018239117949774328, Test Accuracy: 48.19%\n",
            "Epoch 131/300, Loss: 0.00018069198807528744, Test Accuracy: 48.20%\n",
            "Epoch 132/300, Loss: 0.00017898491362525166, Test Accuracy: 48.15%\n",
            "Epoch 133/300, Loss: 0.00017732723691532454, Test Accuracy: 48.18%\n",
            "Epoch 134/300, Loss: 0.00017569279477967163, Test Accuracy: 48.13%\n",
            "Epoch 135/300, Loss: 0.00017416837849964177, Test Accuracy: 48.12%\n",
            "Epoch 136/300, Loss: 0.0001725643589605994, Test Accuracy: 48.11%\n",
            "Epoch 137/300, Loss: 0.00017104406217135855, Test Accuracy: 48.19%\n",
            "Epoch 138/300, Loss: 0.00016945795395679464, Test Accuracy: 48.15%\n",
            "Epoch 139/300, Loss: 0.00016805077483499545, Test Accuracy: 48.16%\n",
            "Epoch 140/300, Loss: 0.00016655272541037222, Test Accuracy: 48.14%\n",
            "Epoch 141/300, Loss: 0.00016510353334285978, Test Accuracy: 48.16%\n",
            "Epoch 142/300, Loss: 0.00016363110967104172, Test Accuracy: 48.14%\n",
            "Epoch 143/300, Loss: 0.00016227884375384036, Test Accuracy: 48.16%\n",
            "Epoch 144/300, Loss: 0.0001609177169352521, Test Accuracy: 48.15%\n",
            "Epoch 145/300, Loss: 0.0001596194585916991, Test Accuracy: 48.15%\n",
            "Epoch 146/300, Loss: 0.00015824802665930925, Test Accuracy: 48.07%\n",
            "Epoch 147/300, Loss: 0.00015700761854699758, Test Accuracy: 48.10%\n",
            "Epoch 148/300, Loss: 0.00015573467623228052, Test Accuracy: 48.13%\n",
            "Epoch 149/300, Loss: 0.0001544452596482477, Test Accuracy: 48.14%\n",
            "Epoch 150/300, Loss: 0.000153129617477833, Test Accuracy: 48.12%\n",
            "Epoch 151/300, Loss: 0.00015191444289393578, Test Accuracy: 48.11%\n",
            "Epoch 152/300, Loss: 0.0001507537609312349, Test Accuracy: 48.14%\n",
            "Epoch 153/300, Loss: 0.0001495489945046196, Test Accuracy: 48.15%\n",
            "Epoch 154/300, Loss: 0.00014845534217342352, Test Accuracy: 48.11%\n",
            "Epoch 155/300, Loss: 0.0001472544481971899, Test Accuracy: 48.09%\n",
            "Epoch 156/300, Loss: 0.0001461544503115674, Test Accuracy: 48.10%\n",
            "Epoch 157/300, Loss: 0.0001450924654546921, Test Accuracy: 48.11%\n",
            "Epoch 158/300, Loss: 0.0001439464388405665, Test Accuracy: 48.14%\n",
            "Epoch 159/300, Loss: 0.0001428904374498309, Test Accuracy: 48.12%\n",
            "Epoch 160/300, Loss: 0.0001418327496697133, Test Accuracy: 48.12%\n",
            "Epoch 161/300, Loss: 0.0001407335506497889, Test Accuracy: 48.14%\n",
            "Epoch 162/300, Loss: 0.00013972097826415973, Test Accuracy: 48.10%\n",
            "Epoch 163/300, Loss: 0.00013875592457116326, Test Accuracy: 48.10%\n",
            "Epoch 164/300, Loss: 0.00013777191270724796, Test Accuracy: 48.14%\n",
            "Epoch 165/300, Loss: 0.00013675116658857557, Test Accuracy: 48.14%\n",
            "Epoch 166/300, Loss: 0.0001357488134180865, Test Accuracy: 48.10%\n",
            "Epoch 167/300, Loss: 0.00013479207320260514, Test Accuracy: 48.11%\n",
            "Epoch 168/300, Loss: 0.00013388558086445786, Test Accuracy: 48.11%\n",
            "Epoch 169/300, Loss: 0.00013296152551910053, Test Accuracy: 48.11%\n",
            "Epoch 170/300, Loss: 0.00013206848796277253, Test Accuracy: 48.09%\n",
            "Epoch 171/300, Loss: 0.00013109411333514582, Test Accuracy: 48.14%\n",
            "Epoch 172/300, Loss: 0.00013023575871511727, Test Accuracy: 48.14%\n",
            "Epoch 173/300, Loss: 0.00012928267314494797, Test Accuracy: 48.15%\n",
            "Epoch 174/300, Loss: 0.00012846737007866867, Test Accuracy: 48.12%\n",
            "Epoch 175/300, Loss: 0.0001276128810927576, Test Accuracy: 48.13%\n",
            "Epoch 176/300, Loss: 0.00012676795985611062, Test Accuracy: 48.10%\n",
            "Epoch 177/300, Loss: 0.00012594569416157932, Test Accuracy: 48.11%\n",
            "Epoch 178/300, Loss: 0.00012510556509033884, Test Accuracy: 48.10%\n",
            "Epoch 179/300, Loss: 0.00012427318890572564, Test Accuracy: 48.12%\n",
            "Epoch 180/300, Loss: 0.0001234359578255928, Test Accuracy: 48.07%\n",
            "Epoch 181/300, Loss: 0.0001227543981582463, Test Accuracy: 48.10%\n",
            "Epoch 182/300, Loss: 0.00012189845085256957, Test Accuracy: 48.11%\n",
            "Epoch 183/300, Loss: 0.00012109196883982232, Test Accuracy: 48.15%\n",
            "Epoch 184/300, Loss: 0.00012033336459263839, Test Accuracy: 48.10%\n",
            "Epoch 185/300, Loss: 0.00011957789834773333, Test Accuracy: 48.09%\n",
            "Epoch 186/300, Loss: 0.00011889524544680723, Test Accuracy: 48.12%\n",
            "Epoch 187/300, Loss: 0.00011817670150385364, Test Accuracy: 48.11%\n",
            "Epoch 188/300, Loss: 0.00011741356272207587, Test Accuracy: 48.12%\n",
            "Epoch 189/300, Loss: 0.00011663876329650698, Test Accuracy: 48.12%\n",
            "Epoch 190/300, Loss: 0.00011601649568473827, Test Accuracy: 48.10%\n",
            "Epoch 191/300, Loss: 0.00011529114390103531, Test Accuracy: 48.10%\n",
            "Epoch 192/300, Loss: 0.00011460761531207615, Test Accuracy: 48.11%\n",
            "Epoch 193/300, Loss: 0.00011393913362109011, Test Accuracy: 48.11%\n",
            "Epoch 194/300, Loss: 0.00011327400709737314, Test Accuracy: 48.09%\n",
            "Epoch 195/300, Loss: 0.00011255473731480338, Test Accuracy: 48.10%\n",
            "Epoch 196/300, Loss: 0.00011193699144069475, Test Accuracy: 48.10%\n",
            "Epoch 197/300, Loss: 0.00011128831962546571, Test Accuracy: 48.06%\n",
            "Epoch 198/300, Loss: 0.00011061448206507165, Test Accuracy: 48.07%\n",
            "Epoch 199/300, Loss: 0.00010996648343186855, Test Accuracy: 48.06%\n",
            "Epoch 200/300, Loss: 0.00010936126477718039, Test Accuracy: 48.09%\n",
            "Epoch 201/300, Loss: 0.00010872979636554674, Test Accuracy: 48.09%\n",
            "Epoch 202/300, Loss: 0.00010805238237754877, Test Accuracy: 48.06%\n",
            "Epoch 203/300, Loss: 0.00010745325176867133, Test Accuracy: 48.08%\n",
            "Epoch 204/300, Loss: 0.00010688551024610748, Test Accuracy: 48.08%\n",
            "Epoch 205/300, Loss: 0.0001062818211182775, Test Accuracy: 48.10%\n",
            "Epoch 206/300, Loss: 0.00010574373207317581, Test Accuracy: 48.08%\n",
            "Epoch 207/300, Loss: 0.00010514277245939718, Test Accuracy: 48.06%\n",
            "Epoch 208/300, Loss: 0.00010454398515771173, Test Accuracy: 48.07%\n",
            "Epoch 209/300, Loss: 0.00010398500439203246, Test Accuracy: 48.03%\n",
            "Epoch 210/300, Loss: 0.00010341250274294775, Test Accuracy: 48.06%\n",
            "Epoch 211/300, Loss: 0.00010286509165064137, Test Accuracy: 48.02%\n",
            "Epoch 212/300, Loss: 0.0001022922982533458, Test Accuracy: 48.02%\n",
            "Epoch 213/300, Loss: 0.0001017616480381462, Test Accuracy: 48.04%\n",
            "Epoch 214/300, Loss: 0.00010120722419587596, Test Accuracy: 48.06%\n",
            "Epoch 215/300, Loss: 0.00010065919746352192, Test Accuracy: 48.06%\n",
            "Epoch 216/300, Loss: 0.00010015197208562957, Test Accuracy: 48.09%\n",
            "Epoch 217/300, Loss: 9.962264697337027e-05, Test Accuracy: 48.06%\n",
            "Epoch 218/300, Loss: 9.912071650721098e-05, Test Accuracy: 48.03%\n",
            "Epoch 219/300, Loss: 9.861386175742891e-05, Test Accuracy: 48.05%\n",
            "Epoch 220/300, Loss: 9.807797728442442e-05, Test Accuracy: 48.07%\n",
            "Epoch 221/300, Loss: 9.759898781370129e-05, Test Accuracy: 48.06%\n",
            "Epoch 222/300, Loss: 9.707719809489288e-05, Test Accuracy: 48.08%\n",
            "Epoch 223/300, Loss: 9.659287619447098e-05, Test Accuracy: 48.01%\n",
            "Epoch 224/300, Loss: 9.610176198860972e-05, Test Accuracy: 48.05%\n",
            "Epoch 225/300, Loss: 9.562645487281969e-05, Test Accuracy: 48.04%\n",
            "Epoch 226/300, Loss: 9.516490555129425e-05, Test Accuracy: 48.08%\n",
            "Epoch 227/300, Loss: 9.464333092361864e-05, Test Accuracy: 48.06%\n",
            "Epoch 228/300, Loss: 9.41896122769935e-05, Test Accuracy: 48.07%\n",
            "Epoch 229/300, Loss: 9.373113361557045e-05, Test Accuracy: 48.05%\n",
            "Epoch 230/300, Loss: 9.325051415461084e-05, Test Accuracy: 48.10%\n",
            "Epoch 231/300, Loss: 9.280621216125985e-05, Test Accuracy: 48.06%\n",
            "Epoch 232/300, Loss: 9.23709645164331e-05, Test Accuracy: 48.04%\n",
            "Epoch 233/300, Loss: 9.191690420354688e-05, Test Accuracy: 48.05%\n",
            "Epoch 234/300, Loss: 9.149982799402818e-05, Test Accuracy: 48.06%\n",
            "Epoch 235/300, Loss: 9.104241520453414e-05, Test Accuracy: 48.12%\n",
            "Epoch 236/300, Loss: 9.062187602556631e-05, Test Accuracy: 48.06%\n",
            "Epoch 237/300, Loss: 9.015128787104239e-05, Test Accuracy: 48.10%\n",
            "Epoch 238/300, Loss: 8.975798034601724e-05, Test Accuracy: 48.11%\n",
            "Epoch 239/300, Loss: 8.932916909685545e-05, Test Accuracy: 48.07%\n",
            "Epoch 240/300, Loss: 8.891844972694567e-05, Test Accuracy: 48.12%\n",
            "Epoch 241/300, Loss: 8.84989285710589e-05, Test Accuracy: 48.10%\n",
            "Epoch 242/300, Loss: 8.809048186606142e-05, Test Accuracy: 48.11%\n",
            "Epoch 243/300, Loss: 8.76758871921794e-05, Test Accuracy: 48.08%\n",
            "Epoch 244/300, Loss: 8.728367433218423e-05, Test Accuracy: 48.08%\n",
            "Epoch 245/300, Loss: 8.691856500666634e-05, Test Accuracy: 48.09%\n",
            "Epoch 246/300, Loss: 8.646230136227326e-05, Test Accuracy: 48.10%\n",
            "Epoch 247/300, Loss: 8.60776418144621e-05, Test Accuracy: 48.09%\n",
            "Epoch 248/300, Loss: 8.567092330999973e-05, Test Accuracy: 48.07%\n",
            "Epoch 249/300, Loss: 8.530815587456798e-05, Test Accuracy: 48.10%\n",
            "Epoch 250/300, Loss: 8.493704192188394e-05, Test Accuracy: 48.09%\n",
            "Epoch 251/300, Loss: 8.456073303787608e-05, Test Accuracy: 48.08%\n",
            "Epoch 252/300, Loss: 8.416605333148561e-05, Test Accuracy: 48.08%\n",
            "Epoch 253/300, Loss: 8.381470049330646e-05, Test Accuracy: 48.12%\n",
            "Epoch 254/300, Loss: 8.343053499488787e-05, Test Accuracy: 48.10%\n",
            "Epoch 255/300, Loss: 8.309112059497882e-05, Test Accuracy: 48.09%\n",
            "Epoch 256/300, Loss: 8.273259066352895e-05, Test Accuracy: 48.10%\n",
            "Epoch 257/300, Loss: 8.235051667822959e-05, Test Accuracy: 48.06%\n",
            "Epoch 258/300, Loss: 8.200056576881702e-05, Test Accuracy: 48.06%\n",
            "Epoch 259/300, Loss: 8.163535884049408e-05, Test Accuracy: 48.12%\n",
            "Epoch 260/300, Loss: 8.129223477869904e-05, Test Accuracy: 48.11%\n",
            "Epoch 261/300, Loss: 8.094448255963547e-05, Test Accuracy: 48.06%\n",
            "Epoch 262/300, Loss: 8.059321076133291e-05, Test Accuracy: 48.08%\n",
            "Epoch 263/300, Loss: 8.027757972933988e-05, Test Accuracy: 48.08%\n",
            "Epoch 264/300, Loss: 7.992413525235318e-05, Test Accuracy: 48.11%\n",
            "Epoch 265/300, Loss: 7.957815061383974e-05, Test Accuracy: 48.07%\n",
            "Epoch 266/300, Loss: 7.925632408415395e-05, Test Accuracy: 48.06%\n",
            "Epoch 267/300, Loss: 7.893767308984189e-05, Test Accuracy: 48.08%\n",
            "Epoch 268/300, Loss: 7.859168363094833e-05, Test Accuracy: 48.05%\n",
            "Epoch 269/300, Loss: 7.827477246916057e-05, Test Accuracy: 48.07%\n",
            "Epoch 270/300, Loss: 7.794570762318707e-05, Test Accuracy: 48.08%\n",
            "Epoch 271/300, Loss: 7.761689709590608e-05, Test Accuracy: 48.04%\n",
            "Epoch 272/300, Loss: 7.728946997814765e-05, Test Accuracy: 48.07%\n",
            "Epoch 273/300, Loss: 7.700191035407034e-05, Test Accuracy: 48.07%\n",
            "Epoch 274/300, Loss: 7.666732798837075e-05, Test Accuracy: 48.07%\n",
            "Epoch 275/300, Loss: 7.636849898593766e-05, Test Accuracy: 48.06%\n",
            "Epoch 276/300, Loss: 7.606172029455992e-05, Test Accuracy: 48.05%\n",
            "Epoch 277/300, Loss: 7.57546789087592e-05, Test Accuracy: 48.08%\n",
            "Epoch 278/300, Loss: 7.546960203530574e-05, Test Accuracy: 48.06%\n",
            "Epoch 279/300, Loss: 7.514341316519831e-05, Test Accuracy: 48.05%\n",
            "Epoch 280/300, Loss: 7.486784440073385e-05, Test Accuracy: 48.05%\n",
            "Epoch 281/300, Loss: 7.45736560592303e-05, Test Accuracy: 48.07%\n",
            "Epoch 282/300, Loss: 7.42616012057921e-05, Test Accuracy: 48.06%\n",
            "Epoch 283/300, Loss: 7.396351445577594e-05, Test Accuracy: 48.04%\n",
            "Epoch 284/300, Loss: 7.369599887528342e-05, Test Accuracy: 48.07%\n",
            "Epoch 285/300, Loss: 7.338809637780425e-05, Test Accuracy: 48.07%\n",
            "Epoch 286/300, Loss: 7.315844634128086e-05, Test Accuracy: 48.06%\n",
            "Epoch 287/300, Loss: 7.284259148048874e-05, Test Accuracy: 48.05%\n",
            "Epoch 288/300, Loss: 7.255652187756631e-05, Test Accuracy: 48.06%\n",
            "Epoch 289/300, Loss: 7.229109432932463e-05, Test Accuracy: 48.07%\n",
            "Epoch 290/300, Loss: 7.201063413476958e-05, Test Accuracy: 48.05%\n",
            "Epoch 291/300, Loss: 7.173385402577507e-05, Test Accuracy: 48.07%\n",
            "Epoch 292/300, Loss: 7.14620932498733e-05, Test Accuracy: 48.04%\n",
            "Epoch 293/300, Loss: 7.120982020821808e-05, Test Accuracy: 48.05%\n",
            "Epoch 294/300, Loss: 7.092329839498691e-05, Test Accuracy: 48.06%\n",
            "Epoch 295/300, Loss: 7.066339547484452e-05, Test Accuracy: 48.07%\n",
            "Epoch 296/300, Loss: 7.039235564835198e-05, Test Accuracy: 48.05%\n",
            "Epoch 297/300, Loss: 7.013239114325675e-05, Test Accuracy: 48.04%\n",
            "Epoch 298/300, Loss: 6.986729420652225e-05, Test Accuracy: 48.07%\n",
            "Epoch 299/300, Loss: 6.962209725127383e-05, Test Accuracy: 48.08%\n",
            "Epoch 300/300, Loss: 6.933938995192437e-05, Test Accuracy: 48.07%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.56      0.56      1000\n",
            "           1       0.63      0.53      0.58      1000\n",
            "           2       0.37      0.37      0.37      1000\n",
            "           3       0.32      0.34      0.33      1000\n",
            "           4       0.41      0.43      0.42      1000\n",
            "           5       0.36      0.35      0.36      1000\n",
            "           6       0.52      0.53      0.53      1000\n",
            "           7       0.53      0.51      0.52      1000\n",
            "           8       0.60      0.65      0.62      1000\n",
            "           9       0.54      0.53      0.53      1000\n",
            "\n",
            "    accuracy                           0.48     10000\n",
            "   macro avg       0.48      0.48      0.48     10000\n",
            "weighted avg       0.48      0.48      0.48     10000\n",
            "\n",
            "time: 2h 55min 24s (started: 2023-12-02 01:42:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AoTM5V-MNxT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}